Locally Nameless Tutorial (part II)
==================================


Questions about `Lec1_full.v`
---------------------------

* What is the definition of `==`/`eq_dec`? What makes it decidable?

* Can you elaborate on the intuition for why `lc_abs_exists` holds?

Questions about `Lec2_full.v`
---------------------------

* What the (#*&$(# is Cofinite quantification?

    * Q for class: [Lec2.v, weakening] Not so much a question as a request —
      could you go over in class why cofinite quantification is needed? I'm
      finding the Coq proof state a little difficult to read and I think I'd
      appreciate seeing it written out in regular math terms. I think it makes
      sense up to where the context in weakening needs to be hypothesized to
      be equal to two appended contexts, but then I'm not sure why the
      cofinite quantification would help (not having actually done the
      exercise yet). It's especially mysterious since it looks like the extra
      argument `L` in `typing_abs` doesn't actually do anything for the
      abstraction aside from being a set that doesn't contain the abstracted
      variable. Would it make sense when written down in a derivation rule, or
      is this purely an artifact of locally nameless?
      
    *  I don’t quite understand the logic behind pick tactic. Is it being used
       because in the typing_abs there is a "forall x" inside the second
       predicate? Should I understand pick to be similar as “exists”?
      
    * Can you please explain the context L in typing_abs. I don’t quite see
      how not specifying it can help.
      
    * Why is it called "cofinite" quantification? And to clarify, is the need
      for cofinite quantification due to some quirk of Coq / the locally
      nameless representation, or is it implicitly needed for pen and paper
      proofs too?

    * Would it be possible to discuss how the proof of `step_lc_exp2` works?
      [From Lec1_full.v] I wasn’t fully certain when we need to invoke the
      tactic “pick fresh x for L” in a proof concerning local closure.

    * What exactly is `exists-fresh`?
    
    * Can you give a precise definition of cofinite quantification? I don't
      immediately see why one would worry that it indicates a bad language
      formalization. ("The use of cofinite quantification may make some people
      worry that we are not formalizing the "right" language" in the
      Exists-Fresh Definition section is the part I'm looking at here.)

* Representing contexts and the weakening proof

    * Would the proof of weakening be any simpler if you defined a partial
      order on contexts (G1 <= G2 iff every x : t in G1 is included in G2) and
      proved that the typing relation respects this order?

    * In Software Foundations (`Stlc.v`), we represent the context as partial
      maps from strings to `typ`s, whereas in `Lec2_full.v` we represent them
      as association lists from `atom`s to `typ`s. Are there advantages to
      using one representation over another?  It’d be interesting to see
      whether using the finite map representation would still result in issues
      in the proof of the weakening lemma where the IH only weakens the
      context at its head (in `Lec2_full`.v).  However, to use the finite map
      representation in PLF, we needed to prove multiple auxiliary lemmas
      about the map data structure (eg. `t_update_eq` in `Map.v` in LF).
      
    * I'm a bit confused by the wording in the Context Equality section; why
      it is a problem that list append is associative?
      
    * Unfortunately, list append is associative, so two Coq expressions 
      may denote the same context even though they are not equal.

General questions
-----------------

* With the help of a tool like LNGen, what is the main pain point of working
  with LN variable representations?
  
* How burdensome are the local closure premises to deal with in larger proofs?
  Could they be taken care of automatically using Coq's typeclass
  infrastructure?
  
* Which lemmas (of the ones that have more direct counterparts) would have
  been harder to prove with a different representation (e.g. only de Brujin
  indices, or PHOAS)?
  
* For CIS6700, my question is: a lot of the other questions ask about how LN
  compares to de Bruijn representations; can we go over what that is?  Also,
  the lecture mentions that the strategy of only working with closed terms
  breaks down when you start doing compiler optimizations; I don't really
  understand how LN with open fixes this.


Comparisons
-----------

1. just use strings for variables

+ closest to what we do "on paper"

+ works fine if substitution is only for closed terms

+ most flexible: easily bind multiple variables in pattern matching, 
  work with multiple sorts of variables, etc.

- adequacy is difficult: need to know that everything that we 
  define works "up-to-alpha-conversion"
  
  i.e. if    G |- t : T   and t =alpha t' then  G |- t' : T
  (and similar results for G and T, if types have bound variables)

- can be difficult to define alpha-equivalence and show that 
  it is an equivalence relation
  
- induction over "syntax" is no helpful as you might want the 
  i.h. to apply to an alpha-equivalent subterm

- "open" substitution is challenging: need to avoid substituting 
  terms with free variables under binders that can capture them.
  Defining this operation is not structurally recursive.
  
2. locally nameless with Ott/LNgen

+ has support for automatic type setting with Ott

+ all alpha-equivalent terms have same representation 
  (no need to define a separate alpha-equivalence relation)
  
+ capture-avoiding substitution is strict structural recursion

- close to what we do "on paper"

- two differnt forms of variables: often need to convert between bound/free
  variables during proofs and need lemmas that talk about that
  
- need to maintain the invariant that all terms are locally closed
  this requires additional lemmas and assumptions in definitions

- all relations should be "equivariant" i.e. stable under renaming 
  free variables. This is easier to prove than "up-to-alpha-conversion"
  but can be tedious to prove for relations specified without the use of
  "cofinite" quantification.

- can have different kinds of variables, but only a single binder. 
  need to be clever if you want to include pattern matching.

- inductive relations work well, but defining functions over 
  syntax can be difficult.

3. de bruijn indices

+ most popular approach in Coq

+ supported by Autosubst library

+ fits well with a categorical semantics that represent terms as 
  arrows from contexts (tuple types) to types (result type)

+ all alpha-equivalent terms have same representation 
  (no need to define a separate alpha-equivalence relation)
  
+ no need for "local closure" predicate. All terms are valid.

+/- capture-avoiding substitution requires shifting

- further away from "paper" proofs because you need to remember to explicitly
  shift terms if you move them from one context to another.  And reason about
  this shifting in your lemma statements/proofs.

- need to carefully track what scope terms belong to. In Agda, we can 
  use dependent types for this, but dependently-typed programming 
  is not so easy to work with in Coq.
  
- Autosubst has limitations with respect to pattern binding, 
  multiple variable sorts
  
- better at defining functions over syntax.

4. PHOAS

Example: 

+ all alpha-equivalent terms have same representation

+ best at defining functions over syntax (i.e. reasoning about compilers)

- least like "paper" proofs

- see: http://adam.chlipala.net/cpdt/html/Cpdt.ProgLang.html


