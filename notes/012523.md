Locally Nameless Tutorial (part II)
==================================


Questions about `Lec1_full.v`
---------------------------

* What is the definition of `==`/`eq_dec`? What makes it decidable?

* Can you elaborate on the intuition for why `lc_abs_exists` holds?

Questions about `Lec2_full.v`
---------------------------

* What the (#*&$(# is Cofinite quantification?

    * Q for class: [Lec2.v, weakening] Not so much a question as a request —
      could you go over in class why cofinite quantification is needed? I'm
      finding the Coq proof state a little difficult to read and I think I'd
      appreciate seeing it written out in regular math terms. I think it makes
      sense up to where the context in weakening needs to be hypothesized to
      be equal to two appended contexts, but then I'm not sure why the
      cofinite quantification would help (not having actually done the
      exercise yet). It's especially mysterious since it looks like the extra
      argument `L` in `typing_abs` doesn't actually do anything for the
      abstraction aside from being a set that doesn't contain the abstracted
      variable. Would it make sense when written down in a derivation rule, or
      is this purely an artifact of locally nameless?
      
    *  I don’t quite understand the logic behind pick tactic. Is it being used
       because in the typing_abs there is a "forall x" inside the second
       predicate? Should I understand pick to be similar as “exists”?
      
    * Can you please explain the context L in typing_abs. I don’t quite see
      how not specifying it can help.
      
    * Why is it called "cofinite" quantification? And to clarify, is the need
      for cofinite quantification due to some quirk of Coq / the locally
      nameless representation, or is it implicitly needed for pen and paper
      proofs too?

    * Would it be possible to discuss how the proof of `step_lc_exp2` works?
      [From Lec1_full.v] I wasn’t fully certain when we need to invoke the
      tactic “pick fresh x for L” in a proof concerning local closure.

    * What exactly is `exists-fresh`?
    
    * Can you give a precise definition of cofinite quantification? I don't
      immediately see why one would worry that it indicates a bad language
      formalization. ("The use of cofinite quantification may make some people
      worry that we are not formalizing the "right" language" in the
      Exists-Fresh Definition section is the part I'm looking at here.)

* Representing contexts and the weakening proof

    * Would the proof of weakening be any simpler if you defined a partial
      order on contexts (G1 <= G2 iff every x : t in G1 is included in G2) and
      proved that the typing relation respects this order?

    * In Software Foundations (`Stlc.v`), we represent the context as partial
      maps from strings to `typ`s, whereas in `Lec2_full.v` we represent them
      as association lists from `atom`s to `typ`s. Are there advantages to
      using one representation over another?  It’d be interesting to see
      whether using the finite map representation would still result in issues
      in the proof of the weakening lemma where the IH only weakens the
      context at its head (in `Lec2_full`.v).  However, to use the finite map
      representation in PLF, we needed to prove multiple auxiliary lemmas
      about the map data structure (eg. `t_update_eq` in `Map.v` in LF).
      
    * I'm a bit confused by the wording in the Context Equality section; why
      it is a problem that list append is associative?

General questions
-----------------

* With the help of a tool like LNGen, what is the main pain point of working
  with LN variable representations?
  
* Which lemmas (of the ones that have more direct counterparts) would have
  been harder to prove with a different representation (e.g. only de Brujin
  indices, or PHOAS)?
  
* For CIS6700, my question is: a lot of the other questions ask about how LN
  compares to de Bruijn representations; can we go over what that is?  Also,
  the lecture mentions that the strategy of only working with closed terms
  breaks down when you start doing compiler optimizations; I don't really
  understand how LN with open fixes this.
